import sys
import re
from BeautifulSoup import BeautifulSoup, NavigableString
import requests
import urllib
import urllib2
import codecs
import operator
from StringIO import StringIO
import gzip

def getPage(url):
    opener = urllib2.build_opener()
    opener.addheaders = [('User-agent', 'MyTestScript/1.0 (contact at myscript@mysite.com)'), ('Accept-encoding', 'gzip')]
    resource = opener.open("http://en.wikipedia.org/wiki/" + url)
    if resource.info().get('Content-Encoding') == 'gzip':
        buf = StringIO( resource.read())
        f = gzip.GzipFile(fileobj=buf)
        return f.read()
    else:
        return resource.read()


def namef(title):
    article = urllib.quote(title)
    opener = urllib2.build_opener()
    opener.addheaders = [('User-agent', 'Mozilla/5.0')] 
    resource = opener.open("http://en.wikipedia.org/wiki/" + article)
    data = resource.read()
    resource.close()
    soup = BeautifulSoup(data)
    concept = soup.find("h1").text
    return concept


def cat(title):
    x=re.compile("^regions|ancient|countries|capitals|boroughs|towns|continents|provinces|numbers|sexual|states|cities|nations|stimulants|drugs|medicines$")
    article = urllib.quote(title)
    data=getPage(article)
    soup = BeautifulSoup(data)
    links=[]
    i=1
    cat=soup.find('div',id="mw-normal-catlinks")
    for link in cat.findAll('a', attrs={'href': re.compile("^/wiki/Category")}):
                line=link.get ('href')[15:]
                line=re.sub('_', ' ',line)
                if x.findall(line.lower())==[]:
                    i*=1
                else:
                    i=0
    return i
    
def alllinks(title):
    i=0
    links=[]
    x=re.compile("^:wikt|portal|index|latin|list|wikipedia|book|wikibooks|image|file|help|template|category|special:|\(disambiguation\)$")
    article=re.sub(' ','_', title)
    try:
        data=getPage(article)
        soup = BeautifulSoup(data)
        para=soup.find('div',id="mw-content-text")
        for link in para.findAll('a', attrs={'href': re.compile("^/wiki/")}):
            line=link.get('href')[6:]
            line=re.sub('_', ' ',line)
            if x.findall(line.lower())==[]:
                if  line.lower() not in (y.lower() for y in links):
                    links.append(line)
                    i=i+1
    except:
        i=0
    return i

def get_links(title):
    x=re.compile("^:wikt|outline|portal|list|sexual|latin|history|glossary|index|book|wikipedia|wikibooks|image|file|help|template|category|special:|english|language|\(disambiguation\)$")
    article = urllib.quote(title)
    opener = urllib2.build_opener()
    opener.addheaders = [('User-agent', 'Mozilla/5.0')] 
    resource = opener.open("http://en.wikipedia.org/wiki/" + article)
    data = resource.read()
    resource.close()
    soup = BeautifulSoup(data)
    links=[]
    links1=[]
    j=0
    for tag in soup.find('div',id="mw-content-text").findAll():
        if tag.name == 'h2':
            break
        if tag.name == 'p':
            for link in tag.findAll('a', attrs={'href': re.compile("^/wiki/")}):
                line=link.get ('href')[6:]            
                line=namef(line)
                line=re.sub('_', ' ',line)
                #line=urllib.unquote(line).decode('utf8')
                #line=line.split('|')[0]
                #line=line.split('#')[0]
                if x.findall(line.lower())==[]:
                    if  line.lower() not in (y.lower() for y in links):
                        links.append(line)
    try:
        link=soup.find('a', {'title':'Edit section: See also'})['href']
        resource2 = opener.open("http://en.wikipedia.org/"+link)
        data2 = resource2.read()
        resource2.close()
        soup2 = BeautifulSoup(data2)
        content= soup2.find('textarea', {"id": "wpTextbox1"}).renderContents()
        m = re.findall(r"\[\[(.*?)\]\]", content)
        for i in m:
            i=urllib.unquote(i).decode('utf8')
            i=i.split('|')[0]
            i=i.split('#')[0]
            i=re.sub('_', ' ', i)
            if x.findall(i.lower())==[]:
                if  i.lower() not in (y.lower() for y in links):
                    links.append(i)

    except:
        j=j+1
    return links


if __name__ == '__main__':
    fw=open("V:\\output.txt","w+")
    title="micro black hole"
    level=1
    i=0
    links=[title]
    link2=[title]
    while (i<level):
        this_level=[]
        for link in links:
            try:
                for k in get_links(link):
                    if k not in this_level:
                         if  k.lower() not in (y.lower() for y in link2):
                             if cat(k)==1:
                                this_level.append(k)                     
            except:
                print 'Couldn\'t pursue %s. Most likely an encoding problem' %(link)
        links=this_level
        pair={}
        for j in links:
            c=urllib.unquote(j)
            n=alllinks(j)
            pair[c] = n
        sorted_pair=sorted(pair.iteritems(), key=operator.itemgetter(1), reverse=True)
        for m in sorted_pair:
            print m[0]
            link2+=[m[0]]
        link2+='='    
        print("====")
        i+=1
    
    for i in link2: fw.write(i+'\n')
    print 'There are %s concepts %s step(s) away from %s' %(len(links),level,title)
    fw.close()
